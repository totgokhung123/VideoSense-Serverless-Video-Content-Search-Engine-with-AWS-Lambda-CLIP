# Worklog - NgÃ y 06/06/2025

## ğŸ“… ThÃ´ng tin cÆ¡ báº£n
- **NgÃ y**: 06/06/2025
- **Thá»©**: Thá»© SÃ¡u
- **Tuáº§n thá»±c táº­p**: Tuáº§n thá»© 4/8
- **Thá»i gian lÃ m viá»‡c**: 9:00 - 17:00
- **Mood**: ğŸš€ Pháº¥n khá»Ÿi vá»›i mÃ´ hÃ¬nh machine learning Ä‘áº§u tiÃªn

## ğŸ¯ Má»¥c tiÃªu ngÃ y hÃ´m nay
- [x] Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i SageMaker Builtâ€‘in Algorithms (XGBoost)

## ğŸ’¼ CÃ´ng viá»‡c Ä‘Ã£ thá»±c hiá»‡n

### 1. TÃ¬m hiá»ƒu SageMaker Built-in Algorithms â±ï¸ 9:00-10:00
- **MÃ´ táº£**:
  - NghiÃªn cá»©u vá» cÃ¡c thuáº­t toÃ¡n built-in cá»§a SageMaker
  - TÃ¬m hiá»ƒu sÃ¢u vá» XGBoost algorithm
  - Äá»c tÃ i liá»‡u vá» hyperparameters quan trá»ng cá»§a XGBoost
  - PhÃ¢n tÃ­ch use cases phÃ¹ há»£p cho XGBoost
  - So sÃ¡nh XGBoost vá»›i cÃ¡c thuáº­t toÃ¡n khÃ¡c
  - NghiÃªn cá»©u vá» input data format requirements
- **Káº¿t quáº£**:
  - Hiá»ƒu rÃµ XGBoost algorithm vÃ  cÃ¡ch triá»ƒn khai trÃªn SageMaker
  - Náº¯m Ä‘Æ°á»£c cÃ¡c hyperparameters quan trá»ng vÃ  tÃ¡c Ä‘á»™ng cá»§a chÃºng
  - Biáº¿t cÃ¡ch chuáº©n bá»‹ dá»¯ liá»‡u cho XGBoost algorithm
  - Hiá»ƒu cÃ¡c trÆ°á»ng há»£p XGBoost phÃ¹ há»£p vÃ  khÃ´ng phÃ¹ há»£p
  - Biáº¿t cÃ¡ch Ä‘Ã¡nh giÃ¡ model XGBoost sau khi train
- **Tools/Tech**:
  - AWS SageMaker documentation
  - XGBoost algorithm
  - Hyperparameter optimization
  - Classification & regression tasks
  - CSV data format
  - Model evaluation metrics

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u train/validation â±ï¸ 10:00-11:30
- **MÃ´ táº£**:
  - Táº¡o thÆ° má»¥c S3 theo chuáº©n SageMaker (train/, validation/)
  - Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tá»« notebook vÃ o S3 theo Ä‘á»‹nh dáº¡ng chuáº©n
  - Split data thÃ nh training vÃ  validation sets
  - Äáº£m báº£o class balance trong cáº£ hai táº­p dá»¯ liá»‡u
  - Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ upload lÃªn S3
  - XÃ¡c nháº­n dá»¯ liá»‡u á»Ÿ Ä‘Ãºng Ä‘á»‹nh dáº¡ng cho XGBoost
- **Káº¿t quáº£**:
  - Dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¢n chia thÃ nh train (80%) vÃ  validation (20%)
  - Files CSV Ä‘Ã£ Ä‘Æ°á»£c upload lÃªn S3 Ä‘Ãºng cáº¥u trÃºc thÆ° má»¥c
  - Data schema vÃ  format phÃ¹ há»£p vá»›i XGBoost requirements
  - Feature columns vÃ  target column Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh rÃµ rÃ ng
  - Dá»¯ liá»‡u Ä‘Æ°á»£c verify trÆ°á»›c khi training
- **Tools/Tech**:
  - AWS S3
  - Train/validation split
  - CSV data format
  - boto3 S3 upload
  - Data stratification
  - Data verification

### 3. Cáº¥u hÃ¬nh XGBoost Estimator â±ï¸ 11:30-13:00
- **MÃ´ táº£**:
  - Import SageMaker XGBoost module
  - XÃ¡c Ä‘á»‹nh instance type phÃ¹ há»£p (ml.m5.xlarge)
  - Cáº¥u hÃ¬nh hyperparameters cÆ¡ báº£n (max_depth, eta, objective, eval_metric)
  - Äá»‹nh nghÄ©a input channels cho train vÃ  validation data
  - Cáº¥u hÃ¬nh output path trÃªn S3
  - Set up IAM role cho XGBoost training job
  - Cáº¥u hÃ¬nh logging vÃ  metrics collection
- **Káº¿t quáº£**:
  - XGBoost estimator Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§
  - Instance type phÃ¹ há»£p vá»›i kÃ­ch thÆ°á»›c dá»¯ liá»‡u
  - Hyperparameters Ä‘Æ°á»£c thiáº¿t láº­p há»£p lÃ½ cho bÃ i toÃ¡n
  - Input channels Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c
  - Output path Ä‘Æ°á»£c thiáº¿t láº­p trÃªn S3
  - IAM permissions Ä‘áº§y Ä‘á»§ cho training job
- **Tools/Tech**:
  - SageMaker Python SDK
  - XGBoost estimator
  - Hyperparameter configuration
  - Instance type selection
  - IAM roles vÃ  policies
  - S3 input/output configuration

### 4. Submit Training Job â±ï¸ 13:30-14:30
- **MÃ´ táº£**:
  - Khá»Ÿi cháº¡y XGBoost training job
  - Monitor training job trÃªn SageMaker console
  - Theo dÃµi logs trong thá»i gian thá»±c
  - Quan sÃ¡t viá»‡c sá»­ dá»¥ng resources (CPU, memory)
  - XÃ¡c nháº­n job status vÃ  progress
  - Validate job completion vÃ  artifact creation
  - Review training time vÃ  cost
- **Káº¿t quáº£**:
  - Training job Ä‘Æ°á»£c submit vÃ  cháº¡y thÃ nh cÃ´ng
  - Job logs Ä‘Æ°á»£c theo dÃµi realtime
  - Resource utilization trong ngÆ°á»¡ng dá»± kiáº¿n
  - Training completed trong khoáº£ng 25 phÃºt
  - Model artifacts Ä‘Æ°á»£c lÆ°u vÃ o S3 location
  - KhÃ´ng cÃ³ errors hay performance issues
- **Tools/Tech**:
  - SageMaker Training Jobs
  - CloudWatch logs
  - SageMaker console
  - Resource monitoring
  - Job tracking
  - S3 artifact storage

### 5. Kiá»ƒm tra logs vÃ  metrics â±ï¸ 14:30-15:30
- **MÃ´ táº£**:
  - Truy cáº­p CloudWatch logs cá»§a training job
  - PhÃ¢n tÃ­ch training vÃ  validation metrics (accuracy, AUC)
  - Check for overfitting/underfitting signs
  - Review feature importance tá»« logs
  - Evaluate convergence cá»§a training process
  - Compare training vÃ  validation performance
  - Identify potential issues tá»« logs
- **Káº¿t quáº£**:
  - Model Ä‘áº¡t accuracy 87% trÃªn validation set
  - AUC score lÃ  0.92, indicating good classification
  - KhÃ´ng cÃ³ dáº¥u hiá»‡u overfitting rÃµ rÃ ng
  - Feature importance Ä‘Æ°á»£c extracted tá»« logs
  - Training process há»™i tá»¥ sau 100 rounds
  - Má»™t sá»‘ warnings vá» missing values Ä‘Æ°á»£c ghi nháº­n
- **Tools/Tech**:
  - CloudWatch logs
  - Machine learning metrics
  - AUC vÃ  accuracy evaluation
  - Overfitting detection
  - Feature importance
  - Convergence analysis

### 6. Deploy model endpoint â±ï¸ 15:30-17:00
- **MÃ´ táº£**:
  - Táº¡o SageMaker model tá»« training artifacts
  - Configure endpoint vá»›i instance type t2.small
  - Deploy model endpoint vá»›i initial instance count lÃ  1
  - Kiá»ƒm tra endpoint status sau deployment
  - Test inference vá»›i sample payload JSON
  - Measure response time cá»§a predictions
  - Document endpoint configuration vÃ  access pattern
- **Káº¿t quáº£**:
  - Model endpoint deployed thÃ nh cÃ´ng
  - t2.small instance cháº¡y á»•n Ä‘á»‹nh cho test workload
  - Endpoint status lÃ  InService
  - Predictions Ä‘Æ°á»£c tráº£ vá» chÃ­nh xÃ¡c tá»« sample payload
  - Response time dÆ°á»›i 200ms cho single predictions
  - Documentation Ä‘áº§y Ä‘á»§ vá» cÃ¡ch sá»­ dá»¥ng endpoint
- **Tools/Tech**:
  - SageMaker model deployment
  - Endpoint configuration
  - Instance selection
  - REST API testing
  - JSON payload format
  - Performance monitoring

## ğŸ“š Kiáº¿n thá»©c há»c Ä‘Æ°á»£c

### ğŸ”§ Technical Skills
- **XGBoost on SageMaker**:
  - Training job configuration
  - Hyperparameter selection
  - Instance type considerations
  - Data format requirements
  - Performance monitoring
  - Model artifacts management
- **Data Preparation**:
  - SageMaker data format standards
  - Training/validation split best practices
  - S3 folder structure requirements
  - Data preprocessing cho XGBoost
  - CSV format optimization
  - Feature normalization impacts
- **Model Deployment**:
  - Endpoint configuration
  - Instance selection criteria
  - Scaling considerations
  - Inference payload formatting
  - Performance optimization
  - Monitoring configurations

### ğŸ’¡ Concepts & Theory
- **Gradient Boosting**: How XGBoost improves on traditional gradient boosting
- **Hyperparameter Tuning**: Impact cá»§a cÃ¡c hyperparameters khÃ¡c nhau trÃªn model performance
- **Evaluation Metrics**: Tradeoffs giá»¯a cÃ¡c metrics nhÆ° accuracy, AUC, precision vÃ  recall
- **Training-Serving Skew**: PhÃ²ng ngá»«a sá»± khÃ¡c biá»‡t giá»¯a training vÃ  inference environments

### ğŸ¤ Soft Skills
- **Experimental Documentation**: Ghi chÃ©p Ä‘áº§y Ä‘á»§ vá» training experiments
- **Performance Analysis**: PhÃ¢n tÃ­ch logs vÃ  metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ model
- **Resource Management**: CÃ¢n báº±ng giá»¯a instance size, cost vÃ  performance
- **Technical Troubleshooting**: Debugging training job issues tá»« logs

## ğŸš§ KhÃ³ khÄƒn vÃ  giáº£i phÃ¡p

### Váº¥n Ä‘á» 1: Äá»‹nh dáº¡ng dá»¯ liá»‡u khÃ´ng tÆ°Æ¡ng thÃ­ch
- **MÃ´ táº£**: XGBoost training job failed vá»›i error vá» input data format
- **Impact**: Training khÃ´ng thá»ƒ báº¯t Ä‘áº§u, delay tiáº¿n Ä‘á»™
- **Root Cause**: SageMaker XGBoost yÃªu cáº§u label column á»Ÿ first column, khÃ´ng nhÆ° documentation má»›i nháº¥t
- **Solution**: Restructure CSV files Ä‘á»ƒ Ä‘áº·t label column á»Ÿ vá»‹ trÃ­ Ä‘áº§u tiÃªn
- **Result**: Training job cháº¡y thÃ nh cÃ´ng sau khi fix data format
- **Lesson**: Verify SageMaker algorithm version vÃ  specific requirements

### Váº¥n Ä‘á» 2: Memory issues trong training
- **MÃ´ táº£**: Training job OOM errors vá»›i dataset lá»›n
- **Impact**: Job failures vÃ  incomplete training
- **Root Cause**: Default instance type khÃ´ng Ä‘á»§ memory cho dataset size
- **Solution**: Upgrade instance type tá»« ml.m4.xlarge lÃªn ml.m5.2xlarge
- **Result**: Training completed successfully vá»›i instance type lá»›n hÆ¡n
- **Lesson**: Estimate memory requirements dá»±a trÃªn dataset size vÃ  algorithm

## ğŸ’­ Reflection & Insights

### What went well today?
- Successfully trained first ML model trÃªn SageMaker
- Effective data preparation vÃ  structured approach
- Good metrics tá»« initial model attempt

### What could be improved?
- Need more systematic hyperparameter exploration
- Should automate data preparation workflow
- Better documentation cá»§a experiment results

### Key Insights
- XGBoost extremely powerful cho structured data problems
- Instance selection critical cho training performance vÃ  cost
- CloudWatch logs provide detailed insights vÃ o training process
- Data format requirements vary by algorithm vÃ  version

### Questions & Curiosities
- How to optimize XGBoost hyperparameters systematically?
- What techniques exist cho interpretability cá»§a XGBoost models?
- Best practices cho A/B testing different models?
- Production strategies cho model monitoring vÃ  retraining?

## ğŸ“‹ Káº¿ hoáº¡ch ngÃ y mai

### Priority Tasks
- [ ] **High**: Implement SageMaker Pipelines cho automated workflow
- [ ] **High**: Explore hyperparameter tuning vá»›i SageMaker HPO
- [ ] **Medium**: Compare XGBoost vá»›i Linear Learner performance

### Learning Goals
- [ ] Hiá»ƒu SageMaker Pipeline components vÃ  workflow
- [ ] Náº¯m vá»¯ng hyperparameter tuning best practices
- [ ] TÃ¬m hiá»ƒu vá» model versioning vÃ  registry

### Meetings & Deadlines
- [ ] Team meeting: present initial model results
- [ ] Submit weekly progress report

## ğŸ“Š Self Assessment

### Productivity
- **Score**: 9/10
- **Reason**: Completed full training workflow tá»« data prep Ä‘áº¿n deployment
- **Improvement**: Cáº§n automation cho repetitive tasks

### Learning
- **Score**: 8/10
- **New Knowledge**: XGBoost implementation, SageMaker training jobs, endpoint deployment
- **Application**: Applied ML theory vÃ o practical model training vÃ  deployment

### Overall Satisfaction
- **Score**: 8/10
- **Highlights**: End-to-end ML workflow implementation
- **Areas for Growth**: Hyperparameter tuning vÃ  model optimization
  
---
## ğŸ“ Attachments & Links

### Learning Resources
- [ AWS lab 1](https://www.youtube.com/watch?v=Le-A72NjaWs&pp=ygURbGFiIEFXUyBTYWdlTWFrZXI%3D) 
- [ AWS lab 2](https://www.youtube.com/watch?v=3_K-yfgc4eE&pp=ygUqIFNhZ2VNYWtlciBCdWlsdOKAkWluIEFsZ29yaXRobXMgKFhHQm9vc3Qp)

---

*Worklog created by: Chu Tien Binh - FCJ Intern Batch 2025*  
*Next review: 07/06/2025*
